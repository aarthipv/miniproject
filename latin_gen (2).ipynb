{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TS-DbJFKt3L"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmeYEZtVLo3i"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # Select your local `damaged_latin_dataset.jsonl`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoHTDNU1LCsG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"damaged_latin_dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "print(f\"Total lines: {len(lines)}\")\n",
        "\n",
        "for i, line in enumerate(lines[:5]):  # check first 5 lines\n",
        "    try:\n",
        "        data = json.loads(line)\n",
        "        print(f\"Line {i+1} OK:\", data)\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Line {i+1} NOT valid JSON:\", line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRAxnryiMcbo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/damaged_latin_dataset.jsonl\"\n",
        "\n",
        "# Read JSONL lines into Python list\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "print(dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byx4t16zNxOV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"facebook/bart-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess(example):\n",
        "    model_input = tokenizer(\n",
        "        example[\"damaged_text\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            example[\"ground_truth\"],\n",
        "            max_length=max_target_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_input\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmMUQXgm4re3"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "lFRs544-O3em",
        "outputId": "becc59e4-ed6c-4d7a-b918-a85f1c9ccabd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='1512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  12/1512 03:06 < 7:47:05, 0.05 it/s, Epoch 0.02/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17' max='1512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  17/1512 04:35 < 7:37:52, 0.05 it/s, Epoch 0.03/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6crTfvWdPVa8"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # BLEU\n",
        "    bleu_score = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
        "\n",
        "    # Accuracy (Exact Match)\n",
        "    exact_matches = sum([1 for pred, label in zip(decoded_preds, decoded_labels) if pred.strip() == label.strip()])\n",
        "    acc_score = exact_matches / len(decoded_preds)\n",
        "\n",
        "    return {\"bleu\": bleu_score[\"bleu\"], \"accuracy\": acc_score}\n",
        "\n",
        "trainer.compute_metrics = compute_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnpaKmUQPZlN"
      },
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def highlight_insertions(damaged, reconstructed):\n",
        "    matcher = SequenceMatcher(None, damaged.split(), reconstructed.split())\n",
        "    highlighted = []\n",
        "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "        if tag == \"equal\":\n",
        "            highlighted.extend(reconstructed.split()[j1:j2])\n",
        "        elif tag == \"insert\":\n",
        "            highlighted.extend([f\"\\033[94m{word}\\033[0m\" for word in reconstructed.split()[j1:j2]])  # Blue highlight\n",
        "        elif tag == \"replace\":\n",
        "            highlighted.extend([f\"\\033[94m{word}\\033[0m\" for word in reconstructed.split()[j1:j2]])\n",
        "    return \" \".join(highlighted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8RGnxl_EUTm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def reconstruct(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Move inputs to the same device as model\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    output = model.generate(**inputs, max_new_tokens=50)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQDHw04KDGUY"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Replace with your Gemini API Key\n",
        "genai.configure(api_key=\"AIzaSyCLZrcCTgqqQIQiixpEutAZiXhvUQYtUos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzBr9GUBDMEt"
      },
      "outputs": [],
      "source": [
        "def translate_with_gemini(text):\n",
        "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "    prompt = f\"\"\"You are a Latin language expert. Translate the following Latin (possibly with corrupted Cyrillic letters) into English:\n",
        "\n",
        "    Latin: {text}\n",
        "\n",
        "    English:\"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNlZ5LkCAven"
      },
      "outputs": [],
      "source": [
        "# damaged = \"Zаmfіrаkе. nꙋ'lꙋ꙼ аltъ\"\n",
        "# reconstructed = reconstruct(damaged)\n",
        "\n",
        "# print(\"Damaged Text:\", damaged)\n",
        "# print(\"Reconstructed:\", reconstructed)\n",
        "# print(\"Highlighted Insertions:\", highlight_insertions(damaged, reconstructed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7iU8L-lEZHB"
      },
      "outputs": [],
      "source": [
        "damaged = \"Gallia est omnis divisa\"\n",
        "\n",
        "reconstructed = reconstruct(damaged)\n",
        "translation = translate_with_gemini(reconstructed)\n",
        "\n",
        "print(\"🟥 Damaged Text:\", damaged)\n",
        "print(\"🟩 Reconstructed:\", reconstructed)\n",
        "print(\"Highlighted Insertions:\", highlight_insertions(damaged, reconstructed))\n",
        "print(\"🟦 English Translation:\", translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54Q68OKgFPJz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
